{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ef11101",
   "metadata": {},
   "source": [
    "# FocusGAN ECE 285 Final Project\n",
    "based off: https://arxiv.org/pdf/1701.01081.pdf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f29c04cf",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eec11c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddieh00/anaconda3/lib/python3.9/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: 'libc10_cuda.so: cannot open shared object file: No such file or directory'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import vgg16\n",
    "import torch.optim as optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms\n",
    "import torch_directml\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.datasets as datasets\n",
    "import glob\n",
    "import os\n",
    "\n",
    "device = torch_directml.device(torch_directml.default_device())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d7bea26",
   "metadata": {},
   "source": [
    "## Visualize CAT2000 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae369aab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b0df7c61",
   "metadata": {},
   "source": [
    "## Dataloader and Device Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86063142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.img_files = glob.glob(self.root_dir + '/*/*.jpg')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_files[idx]\n",
    "        image = Image.open(img_path).convert('RGB')  # Convert image to RGB\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image\n",
    "\n",
    "class SaliencyMapDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None, map_transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.map_transform = map_transform\n",
    "        self.img_files = glob.glob(self.root_dir + '/*/*.jpg')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_files[idx]\n",
    "        map_path = os.path.join(os.path.dirname(img_path), 'Output', os.path.basename(img_path).replace('.jpg', '_SaliencyMap.jpg'))\n",
    "\n",
    "        image = Image.open(img_path).convert('RGB')  # Convert image to RGB\n",
    "        saliency_map = Image.open(map_path).convert('L')  # Convert saliency map to grayscale\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.map_transform:\n",
    "            saliency_map = self.map_transform(saliency_map)\n",
    "\n",
    "        return image, saliency_map\n",
    "\n",
    "\n",
    "\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "map_transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),  \n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "batch_size = 1\n",
    "saliency_map_dataset = SaliencyMapDataset(root_dir='./trainSet/Stimuli', transform=image_transform, map_transform=map_transform)\n",
    "dataloader = DataLoader(saliency_map_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "739d39a6",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9223f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        vgg = vgg16(pretrained=True)\n",
    "        self.encoder = nn.Sequential(*list(vgg.features.children())[:-1]) # remove last pooling layer\n",
    "        self.decoder = self.make_decoder()\n",
    "        \n",
    "    def make_decoder(self):\n",
    "        decoder = nn.Sequential(\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),  # conv6_1\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),  # conv6_2\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),  # conv6_3\n",
    "        nn.ReLU(),\n",
    "        nn.Upsample(scale_factor=2),  # upsample6\n",
    "\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),  # conv7_1\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),  # conv7_2\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(512, 512, kernel_size=3, padding=1),  # conv7_3\n",
    "        nn.ReLU(),\n",
    "        nn.Upsample(scale_factor=2),  # upsample7\n",
    "\n",
    "        nn.Conv2d(512, 256, kernel_size=3, padding=1),  # conv8_1\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),  # conv8_2\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(256, 256, kernel_size=3, padding=1),  # conv8_3\n",
    "        nn.ReLU(),\n",
    "        nn.Upsample(scale_factor=2),  # upsample8\n",
    "\n",
    "        nn.Conv2d(256, 128, kernel_size=3, padding=1),  # conv9_1\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(128, 128, kernel_size=3, padding=1),  # conv9_2\n",
    "        nn.ReLU(),\n",
    "        nn.Upsample(scale_factor=2),  # upsample9\n",
    "\n",
    "        nn.Conv2d(128, 64, kernel_size=3, padding=1),  # conv10_1\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, padding=1),  # conv10_2\n",
    "        nn.ReLU(),\n",
    "\n",
    "        nn.Conv2d(64, 1, kernel_size=1),  # output\n",
    "        nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        return decoder\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        #this first layer is needed for training. In training, the images and saliency maps\n",
    "        #are concatenated so we get a 4 channel input rataher than 3 chanel rgb\n",
    "        self.first_layer = nn.Conv2d(4, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv_layers = self.make_conv_layers()\n",
    "        self.fc_layers = self.make_fc_layers()\n",
    "\n",
    "    def make_conv_layers(self):\n",
    "        conv_layers = nn.Sequential(\n",
    "            nn.Conv2d(3, 3, kernel_size= 1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(stride=2, kernel_size=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size= 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(stride=2, kernel_size=2, padding=0),\n",
    "\n",
    "            nn.Conv2d(64, 64, kernel_size= 3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(stride=2, kernel_size=2, padding=0)\n",
    "        )\n",
    "        return conv_layers\n",
    "\n",
    "    def make_fc_layers(self):\n",
    "        fc_layers = nn.Sequential(\n",
    "            nn.Linear(64 * 28 * 28, 100),  # fc4\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(100, 2),  # fc5\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(2, 1),  # fc6\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        return fc_layers\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_layer(x)\n",
    "        x = self.conv_layers(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten the tensor\n",
    "        x = self.fc_layers(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0871eada",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddieh00/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eddieh00/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 16, 16])\n",
      "torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "generator = Generator()\n",
    "encoder = generator.encoder\n",
    "decoder = generator.decoder\n",
    "discriminator = Discriminator()\n",
    "\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "discriminator = discriminator.to(device)\n",
    "\n",
    "\n",
    "def get_saliency_map(image):\n",
    "    image = image.unsqueeze(0).to(device)\n",
    "    encoded = encoder(image)\n",
    "    print(encoded.shape)\n",
    "    saliency_map = decoder(encoded)\n",
    "    return saliency_map\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),   # Resize images to 256x256\n",
    "    transforms.ToTensor(),           # Convert images to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with ImageNet mean and std\n",
    "])\n",
    "example = Image.open(r\"./trainSet/Stimuli/Action/001.jpg\")\n",
    "ex = transform(example)\n",
    "sal_map = get_saliency_map(ex)\n",
    "print(sal_map.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e52ddfcc",
   "metadata": {},
   "source": [
    "## produce a single example saliency map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca4ab722",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Ensure we have detached the saliency map and moved it to cpu\n",
    "sal_map = sal_map.detach().cpu().squeeze()\n",
    "\n",
    "# As saliency map has values between 0 and 1, we can directly visualize it\n",
    "plt.imshow(sal_map, cmap='hot')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ef0615b9",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea78765f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddieh00/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eddieh00/anaconda3/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "Images Shape: torch.Size([1, 3, 256, 256])\n",
      "True Saliency Maps Shape: torch.Size([1, 1, 256, 256])\n",
      "test1\n",
      "test1.5\n",
      "torch.Size([1, 512, 16, 16])\n",
      "test2\n",
      "test3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eddieh00/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:3098: UserWarning: The operator 'aten::binary_cross_entropy' is not currently supported on the DML backend and will fall back to run on the CPU. This may have performance implications. (Triggered internally at /home/vsts/work/1/s/pytorch-directml-plugin/torch_directml/csrc/dml/dml_cpu_fallback.cpp:15.)\n",
      "  return torch._C._nn.binary_cross_entropy(input, target, weight, reduction_enum)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [3, 3, 1, 1], expected input[1, 4, 256, 256] to have 3 channels, but got 4 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4865/2134066211.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_4865/2134066211.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, data_loader, epochs)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'test3'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0;31m# Forward pass through the discriminator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mreal_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_saliency_maps\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0mfake_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_saliency_maps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_4865/1170794368.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# flatten the tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [3, 3, 1, 1], expected input[1, 4, 256, 256] to have 3 channels, but got 4 channels instead"
     ]
    }
   ],
   "source": [
    "def train(model, data_loader, epochs=20):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch, (images, true_saliency_maps) in enumerate(data_loader):\n",
    "            images = images.to(device)\n",
    "            true_saliency_maps = true_saliency_maps.to(device)\n",
    "            print(f\"Images Shape: {images.shape}\")\n",
    "            print(f\"True Saliency Maps Shape: {true_saliency_maps.shape}\")\n",
    "\n",
    "            print('test1')\n",
    "            # Forward pass through the encoder and decoder\n",
    "            encoded = encoder(images)\n",
    "            print('test1.5')\n",
    "            print(encoded.shape)\n",
    "            pred_saliency_maps = decoder(encoded)\n",
    "\n",
    "            print('test2')\n",
    "            # Compute the content loss\n",
    "            content_loss = criterion(pred_saliency_maps, true_saliency_maps)\n",
    "            \n",
    "            print('test3')\n",
    "            # Forward pass through the discriminator\n",
    "            real_outputs = discriminator(torch.cat([images, true_saliency_maps], dim=1))\n",
    "            fake_outputs = discriminator(torch.cat([images, pred_saliency_maps.detach()], dim=1))\n",
    "            \n",
    "            print('test4')\n",
    "            # Compute the adversarial loss for the generator\n",
    "            adv_loss_gen = criterion(fake_outputs, Variable(torch.ones(fake_outputs.size())).to(device))\n",
    "            \n",
    "            print('test5')\n",
    "            # Compute the adversarial loss for the discriminator\n",
    "            adv_loss_real = criterion(real_outputs, Variable(torch.ones(real_outputs.size())).to(device))\n",
    "            adv_loss_fake = criterion(fake_outputs, Variable(torch.zeros(fake_outputs.size())).to(device))\n",
    "            adv_loss_dis = (adv_loss_real + adv_loss_fake) / 2\n",
    "            \n",
    "            print('test6')\n",
    "            # Combine the losses for the generator and perform backpropagation\n",
    "            gen_loss = content_loss + adv_loss_gen\n",
    "            encoder.zero_grad()\n",
    "            decoder.zero_grad()\n",
    "            gen_loss.backward()\n",
    "            decoder_optimizer.step()\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "            print('test7')\n",
    "            # Perform backpropagation on the discriminator\n",
    "            discriminator.zero_grad()\n",
    "            adv_loss_dis.backward()\n",
    "            discriminator_optimizer.step()\n",
    "            \n",
    "            print(f\"Batch {batch+1}/{len(data_loader)}, Content Loss: {content_loss.item()}, Adversarial Loss: {adv_loss_dis.item()}\")\n",
    "\n",
    "        print(\"Epoch finished!\")\n",
    "        \n",
    "# Define the model\n",
    "generator = Generator().to(device)\n",
    "encoder = generator.encoder\n",
    "decoder = generator.decoder\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "encoder_optimizer = optim.Adagrad(encoder.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "decoder_optimizer = optim.Adagrad(decoder.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "discriminator_optimizer = optim.Adagrad(discriminator.parameters(), lr=3e-4, weight_decay=1e-4)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "train(generator, dataloader, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cccd182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output Shape: torch.Size([32, 512, 16, 16])\n",
      "torch.Size([32, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "images, maps = next(iter(dataloader))\n",
    "\n",
    "# Move to device\n",
    "images = images.to(device)\n",
    "\n",
    "# Forward pass through the encoder\n",
    "encoded = encoder(images)\n",
    "\n",
    "print(f\"Encoder Output Shape: {encoded.shape}\")\n",
    "\n",
    "print(maps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "612adca6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential(\n",
      "  (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (5): ReLU()\n",
      "  (6): Upsample(scale_factor=2.0, mode='nearest')\n",
      "  (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (8): ReLU()\n",
      "  (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (10): ReLU()\n",
      "  (11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (12): ReLU()\n",
      "  (13): Upsample(scale_factor=2.0, mode='nearest')\n",
      "  (14): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (15): ReLU()\n",
      "  (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (17): ReLU()\n",
      "  (18): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (19): ReLU()\n",
      "  (20): Upsample(scale_factor=2.0, mode='nearest')\n",
      "  (21): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (22): ReLU()\n",
      "  (23): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (24): ReLU()\n",
      "  (25): Upsample(scale_factor=2.0, mode='nearest')\n",
      "  (26): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (27): ReLU()\n",
      "  (28): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (29): ReLU()\n",
      "  (30): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (31): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(decoder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "44e02d28",
   "metadata": {},
   "source": [
    "## U-Net Generator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60d5ab8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "568c9abc",
   "metadata": {},
   "source": [
    "## Discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e03074c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "34179327",
   "metadata": {},
   "source": [
    "## Loss Functions and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5db479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d9681b64",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34872a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "be9097dc",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adaddd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
